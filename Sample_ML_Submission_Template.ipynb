{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Zomato Data Analysis Project\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "Name - Shawn Lasrado\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project analyzes restaurant and review data from Zomato to uncover trends in customer behavior, cuisine popularity, and rating patterns. Two datasets restaurant metadata and customer reviews were cleaned, merged, and explored using 15 visualizations.\n",
        "\n",
        "**Key insights include:**\n",
        "\n",
        "Most common cuisines are North Indian and Chinese, while\n",
        "\n",
        "Mediterranean and European are among the highest rated.\n",
        "\n",
        "No strong link between cost and rating, showing budget restaurants can perform well.\n",
        "\n",
        "Evening reviews tend to have slightly higher ratings.\n",
        "\n",
        "Photos and longer reviews often accompany better ratings, suggesting higher engagement.\n",
        "\n",
        "Some Zomato collections consistently feature higher-rated restaurants.\n",
        "\n",
        "Charts like correlation heatmaps, pair plots, bar plots, and scatterplots helped visualize patterns across numeric and categorical variables.\n",
        "\n",
        "These findings can guide Zomato and restaurant partners to optimize menus, personalize recommendations, and promote high performing categories. Overall, the project highlights how data driven decisions can enhance user satisfaction and restaurant performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zomato collects vast amounts of restaurant and review data, including ratings, cuisines, cost, user reviews, and other metadata. However, it remains a challenge to understand hidden patterns in customer preferences, cuisine performance, and restaurant characteristics across the platform.\n",
        "\n",
        "This project applies exploratory data analysis and unsupervised machine learning techniques to identify natural groupings, patterns, and relationships within Zomato’s data. The objective is to uncover:\n",
        "\n",
        "* Clusters of restaurants based on cost, rating, and cuisine variety\n",
        "\n",
        "* Trends in customer behavior and engagement\n",
        "\n",
        "* Key features that differentiate high-performing restaurants\n",
        "\n",
        "These insights can support Zomato in improving content organization, enhancing restaurant discovery, and enabling data driven strategies for both platform and restaurant partners without relying on labeled or supervised outputs."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data1 = pd.read_csv('/content/drive/MyDrive/Datasets/Zomato_metadata.csv')\n",
        "data2 = pd.read_csv('/content/drive/MyDrive/Datasets/Zomato_reviews.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "data1.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.head()"
      ],
      "metadata": {
        "id": "aIzLjmobD6aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "data1.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.shape"
      ],
      "metadata": {
        "id": "hYEGR5jiD9dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "data1.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.info()"
      ],
      "metadata": {
        "id": "KbL8CsUgEFy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "data1.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.duplicated().sum()"
      ],
      "metadata": {
        "id": "sMXIqbN1EI8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "data1.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.isnull().sum()"
      ],
      "metadata": {
        "id": "HVA8o7CpESVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "missing = data1.isnull().sum()\n",
        "missing = missing[missing > 0]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(x=missing.index, y=missing.values, palette=\"mako\")\n",
        "plt.title(\"Missing Values Count in Reviews Dataset\")\n",
        "plt.ylabel(\"Missing Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing = data2.isnull().sum()\n",
        "missing = missing[missing > 0]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(x=missing.index, y=missing.values, palette=\"mako\")\n",
        "plt.title(\"Missing Values Count in Reviews Dataset\")\n",
        "plt.ylabel(\"Missing Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fVv_70w4E1Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "data1.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.columns"
      ],
      "metadata": {
        "id": "5n7IJOn2E7Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "data1.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.describe()"
      ],
      "metadata": {
        "id": "LL2DDJWEFAaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "data1.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.nunique()"
      ],
      "metadata": {
        "id": "l6j1MfN9FEkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# Drop duplicates from reviews\n",
        "data2.drop_duplicates(inplace=True)\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing critical values in reviews\n",
        "data2.dropna(subset=[\"Reviewer\", \"Review\", \"Rating\", \"Time\"], inplace=True)"
      ],
      "metadata": {
        "id": "W00BF5ixFUAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean the data and convert cost into numerical\n",
        "data1[\"Cost\"] = data1[\"Cost\"].str.replace(\",\", \"\").astype(int)"
      ],
      "metadata": {
        "id": "mhn3Po75FZnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# its Fills missing metadata values\n",
        "data1[\"Collections\"] = data1[\"Collections\"].fillna(\"Not Specified\")\n",
        "data1[\"Timings\"] = data1[\"Timings\"].fillna(\"Not Available\")"
      ],
      "metadata": {
        "id": "6MnTxQ3WFjNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert rating into a numeric value\n",
        "def convert_rating(x):\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "data2[\"Rating\"] = data2[\"Rating\"].apply(convert_rating)\n",
        "data2.dropna(subset=[\"Rating\"], inplace=True)"
      ],
      "metadata": {
        "id": "HYlwKCdTFoCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the two datasets on restaurant name\n",
        "merged_data = pd.merge(data2, data1, left_on=\"Restaurant\", right_on=\"Name\", how=\"inner\")"
      ],
      "metadata": {
        "id": "glOv1mY_F65I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final dataset shape:\", merged_df.shape)\n",
        "merged_data.head()"
      ],
      "metadata": {
        "id": "7Jd1zqHbGMXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 average Cost vs Rating Scatter Plot\n",
        "avg_data = merged_data.groupby('Restaurant')[['Cost', 'Rating']].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=avg_data, x='Cost', y='Rating')\n",
        "plt.title(\"Average Cost vs Average Rating per Restaurant\")\n",
        "plt.xlabel(\"Average Cost for Two\")\n",
        "plt.ylabel(\"Average Rating\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To explore the relationship between pricing of the product and customer satisfaction."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's no strong linear correlation between cost and rating. High ratings are observed across various cost ranges."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Positive Business Impact:\n",
        "Restaurants don’t need to be expensive to achieve high ratings, they can focus on quality service and food.\n",
        "\n",
        "* Negative Growth Insight:\n",
        "Overpricing without first improving customer experience won’t guarantee better ratings or loyalty.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 Restaurants with most cuisines\n",
        "merged_data['Cuisine Count'] = merged_data['Cuisines'].apply(lambda x: len(str(x).split(\", \")))\n",
        "top_cuisine_counts = merged_data[['Restaurant', 'Cuisine Count']].drop_duplicates().sort_values(by='Cuisine Count', ascending=False).head(10)"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=top_cuisine_counts, x='Cuisine Count', y='Restaurant', palette='mako')\n",
        "plt.title(\"Top 10 Restaurants with Most Cuisines\")\n",
        "plt.xlabel(\"Number of Cuisines\")\n",
        "plt.ylabel(\"Restaurant\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JWDNuDOHG_iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "so that we can identify which restaurants offers more diverse cuisine options."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "restaurant called - \"Beyond Flavours\", offers the most variety, potentially attracting more customers."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Positive Business Impact:\n",
        "Diversity in cuisine may help in attracting more customers, it also helps in increasing the customer base.\n",
        "\n",
        "*   Negative Growth Insight:\n",
        "Offering too many cuisines might cause issues with the brand identity cause there is no unique dish.\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 most common cuisines\n",
        "merged_data['Cuisine List'] = merged_data['Cuisines'].str.split(', ')\n",
        "exploded = merged_data.explode('Cuisine List')\n",
        "\n",
        "top_cuisine_freq = exploded['Cuisine List'].value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_cuisine_freq.values, y=top_cuisine_freq.index, palette='Set2')\n",
        "plt.title(\"Top 10 Most Common Cuisines\")\n",
        "plt.xlabel(\"Number of Restaurants\")\n",
        "plt.ylabel(\"Cuisine\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "so that we can find the most popular cuisines across different restaurants."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we found that the north indian and chinese cuisines are the most common one."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Positive Business Impact:\n",
        "New restaurants can tap into these popular cuisines so that they can get the initial traction.\n",
        "\n",
        "* Negative Growth Insight:\n",
        "Entering an oversaturated cuisine market without any uniqueness or anything to differentiate could result in poor visibility and growth.\n",
        "\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 Rating Distribution\n",
        "sns.histplot(merged_data['Rating'], bins=10, kde=True)\n",
        "plt.title(\"Rating Distribution\")\n",
        "plt.xlabel(\"Rating\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand how the review rating is distributed."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the peak is at rating 5, which indicates that there are many satisfied customers."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Positive Business Impact:\n",
        "High ratings boost trust. Companies can leverage this in marketing.\n",
        "\n",
        "* Negative Growth Insight:\n",
        "The spike in 5-star reviews may also indicate possible review manipulation if not organic.\n",
        "\n"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 Average Cost per Cuisine\n",
        "avg_cost = exploded.groupby('Cuisine List')['Cost'].mean().sort_values(ascending=False).head(10)\n",
        "sns.barplot(x=avg_cost.values, y=avg_cost.index)\n",
        "plt.title(\"Top 10 Most Expensive Cuisines\")\n",
        "plt.xlabel(\"Average Cost\")\n",
        "plt.ylabel(\"Cuisine\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand which cuisines are sold at a higher price."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modern Indian, Japanese, and Sushi are the most expensive dishes."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Positive Business Impact:\n",
        "Premium dishes can be targeted to high-income segments for increased revenue.\n",
        "\n",
        "* Negative Growth Insight:\n",
        "Expensive dishes may cause issues to customers that are price sensitive if not backed by quality.\n",
        "\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 Review Length vs Rating\n",
        "merged_data['Review Length'] = merged_data['Review'].str.len()\n",
        "sns.scatterplot(x=merged_data['Review Length'], y=merged_data['Rating'])\n",
        "plt.title(\"Review Length vs Rating\")\n",
        "plt.xlabel(\"Review Length (characters)\")\n",
        "plt.ylabel(\"Rating\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find whether longer reviews correlate with higher/lower ratings."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Longer reviews are common across all ratings, suggesting strong sentiment, it can either be positive or negative."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Positive Business Impact:\n",
        "Detailed reviews offer actionable feedback for improvement.\n",
        "\n",
        "* Negative Growth Insight:\n",
        "Frequent long 1-star reviews may highlight serious issues needing urgent attention.\n",
        "\n"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 Distribution of Cost\n",
        "sns.histplot(merged_data['Cost'], bins=20, kde=True)\n",
        "plt.title(\"Cost Distribution\")\n",
        "plt.xlabel(\"Cost for Two\")\n",
        "plt.ylabel(\"Number of Restaurants\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the cost among different restaurants."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Majority of restaurants are in the ₹300–₹800 range."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Positive Business Impact:\n",
        "Targeting mid-range pricing is ideal to attract the bulk of consumers.\n",
        "\n",
        "* Negative Growth Insight:\n",
        "Restaurants priced too high without strong justification risk low traffic.\n",
        "\n"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 Rating Distribution by Cost Bucket\n",
        "merged_data['Cost Bucket'] = pd.cut(merged_data['Cost'], bins=[0, 500, 1000, 1500, 2000, 3000], labels=['<500','500-1k','1k-1.5k','1.5k-2k','2k+'])\n",
        "sns.boxplot(x=merged_data['Cost Bucket'], y=merged_data['Rating'])\n",
        "plt.title(\"Rating by Cost Bucket\")\n",
        "plt.xlabel(\"Cost Bucket\")\n",
        "plt.ylabel(\"Rating\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare rating distributions across cost brackets."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All cost buckets show similar rating medians, meaning price doesn’t impact ratings."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Positive Business Impact:\n",
        "Lower-cost restaurants can still compete on satisfaction.\n",
        "\n",
        "* Negative Growth Insight:\n",
        "Investing heavily in pricing might not yield better ratings unless accompanied by exceptional experience.\n",
        "\n"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 Number of Pictures Shared per Cuisine\n",
        "pics_per_cuisine = exploded.groupby('Cuisine List')['Pictures'].sum().sort_values(ascending=False).head(10)\n",
        "sns.barplot(x=pics_per_cuisine.values, y=pics_per_cuisine.index)\n",
        "plt.title(\"Most Photographed Cuisines\")\n",
        "plt.xlabel(\"Pictures\")\n",
        "plt.ylabel(\"Cuisine\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify cuisines which is photographed the most."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "North Indian and Chinese is again at top, these are the most photographed cuisines."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Positive Business Impact:\n",
        "the food which are photographed the most can drive organic marketing via social media.\n",
        "\n",
        "* Negative Growth Insight:\n",
        "Cuisines with less visual appeal may struggle for exposure on platforms like Instagram unless creatively presented.\n",
        "\n"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 Average Rating by Time of Day (AM/PM)\n",
        "merged_data['Hour'] = pd.to_datetime(merged_data['Time']).dt.hour\n",
        "merged_data['Time of Day'] = merged_data['Hour'].apply(lambda x: 'AM' if x < 12 else 'PM')\n",
        "sns.boxplot(x=merged_data['Time of Day'], y=merged_data['Rating'])\n",
        "plt.title(\"Rating by Time of Day\")\n",
        "plt.xlabel(\"Time of Day\")\n",
        "plt.ylabel(\"Rating\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test if the time of review affects the rating."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AM and PM ratings show similar medians, with a slightly wider spread in PM."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Positive Business Impact:\n",
        "Consistent ratings across day parts suggest stable service quality.\n",
        "\n",
        "* Negative Growth Insight:\n",
        "Slight increase in low PM ratings may hint at evening rush issues like slow service or wait times.\n",
        "\n"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 common review words\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text = ' '.join(merged_data['Review'].dropna().astype(str))\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=STOPWORDS).generate(text)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Most Frequent Words in Customer Reviews\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To extract qualitative insights from customer feedback using natural language."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Words like food, taste, service, place, ambience, and good, reflecting common themes customers care about."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Positive Business Impact:\n",
        "Helps prioritize what aspects matter most to customers emphasizing food quality, service, and ambiance can improve satisfaction.\n",
        "\n",
        "* Negative Growth Insight:\n",
        "Words like time or price could hint at delays or cost concerns if seen frequently with negative context needs deeper sentiment analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 Average Rating per Hour of the Day\n",
        "merged_data['Hour'] = pd.to_datetime(merged_data['Time']).dt.hour\n",
        "\n",
        "hourly_rating = merged_data.groupby('Hour')['Rating'].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(x=hourly_rating['Hour'], y=hourly_rating['Rating'], marker='o')\n",
        "plt.title(\"Average Rating by Hour of the Day\")\n",
        "plt.xlabel(\"Hour\")\n",
        "plt.ylabel(\"Average Rating\")\n",
        "plt.xticks(range(0, 24))\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To investigate if the time of day affects review sentiment."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ratings peak around 5 AM and dip slightly between 7–8 AM, suggesting early reviewers are more positive."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  Positive Business Impact:\n",
        "Restaurants can focus service improvements during low rated hours and consider using this trend for targeted offers.\n",
        "\n",
        "* Negative Growth Insight:\n",
        "If ratings drop consistently during specific hours, it could indicate service issues during those periods.\n",
        "\n"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 Top 10 Restaurants by Number of Reviews\n",
        "top_reviewed = merged_data['Restaurant'].value_counts().head(10)\n",
        "sns.barplot(x=top_reviewed.values, y=top_reviewed.index)\n",
        "plt.title(\"Top 10 Most Reviewed Restaurants\")\n",
        "plt.xlabel(\"Number of Reviews\")\n",
        "plt.ylabel(\"Restaurant\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To highlight which restaurants generate the highest volume of customer engagement."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restaurants like Beyond Flavours and Paradise lead in number of reviews, showing strong customer interaction and visibility."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Positive Business Impact:\n",
        "These restaurants can be studied as benchmarks for engagement strategies. High review counts boost credibility and SEO visibility.\n",
        "\n",
        "* Negative Growth Insight:\n",
        "High number of reviews might also bring more scrutiny. If many are negative or unresolved, it could damage reputation.\n",
        "\n"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "numeric_df = merged_data[['Cost', 'Rating', 'Pictures', 'Review Length']].dropna()\n",
        "\n",
        "corr_matrix = numeric_df.corr()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Correlation Heatmap: Cost, Rating, Pictures & Review Length\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This heatmap helps us understand the strength and direction of linear relationships between key numeric variables that may influence user satisfaction and revenue, such as cost, review pictures, review length, and ratings."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Cost is weakly positively correlated with Rating, meaning more expensive restaurants may be rated slightly better.\n",
        "\n",
        "* Review Length and Pictures show a moderate positive correlation (0.47), suggesting that users who post pictures tend to write longer reviews.\n",
        "\n",
        "* Rating has almost no correlation with Review Length (-0.03) and a very weak positive correlation with Pictures (0.08), indicating that more review content doesn’t necessarily mean a higher rating.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "pair_data = merged_data[['Cost', 'Rating']].dropna()\n",
        "\n",
        "sns.pairplot(pair_data, diag_kind='kde', corner=True)\n",
        "\n",
        "plt.suptitle(\"Pair Plot: Cost vs Rating\", y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot helps visualize both the distribution and the relationship between Cost and Rating using scatter plots and histograms. It supports further investigation into whether cost influences customer satisfaction."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* Most restaurants fall into the lower cost range (below ₹1000), based on the distribution.\n",
        "\n",
        "* Ratings are clustered around 3 to 5 regardless of cost, showing no strong linear pattern.\n",
        "\n",
        "* While higher-cost restaurants exist, they don’t consistently receive higher ratings.\n",
        "\n",
        "* This supports the correlation heatmap’s conclusion: cost does not strongly influence rating.\n",
        "\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis -\n",
        "* H₀: There is no difference in average rating across restaurants with different counts of review pictures.\n",
        "\n",
        "Alternate Hypothesis -\n",
        "* H₁: Restaurants with more review pictures have significantly different average ratings."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "merged_data['Picture_Group'] = pd.cut(\n",
        "    merged_data['Pictures'],\n",
        "    bins=[-1, 0, 2, 5, float('inf')],\n",
        "    labels=['0', '1-2', '3-5', '6+']\n",
        ")\n",
        "\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "group_0 = merged_data[merged_data['Picture_Group'] == '0']['Rating']\n",
        "group_1_2 = merged_data[merged_data['Picture_Group'] == '1-2']['Rating']\n",
        "group_3_5 = merged_data[merged_data['Picture_Group'] == '3-5']['Rating']\n",
        "group_6_plus = merged_data[merged_data['Picture_Group'] == '6+']['Rating']\n",
        "\n",
        "f_stat, p_val = f_oneway(group_0, group_1_2, group_3_5, group_6_plus)\n",
        "\n",
        "print(f\"F-Statistic: {f_stat:.3f}\")\n",
        "print(f\"P-Value: {p_val:.4f}\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Used: One-Way ANOVA"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   ANOVA is suitable for comparing the means of more than two independent groups.\n",
        "*   It helps test whether at least one group mean is different without doing multiple t-tests.\n",
        "\n"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis -\n",
        "*   H₀: Less popular cuisines (by count) have the same average rating as popular cuisines.\n",
        "\n",
        "Alternate Hypothesis -\n",
        "*   H₁: Less popular cuisines have a different (possibly higher) average rating than popular cuisines."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# Step 1: Count cuisines\n",
        "cuisine_counts = merged_data['Cuisines'].value_counts()\n",
        "median_count = cuisine_counts.median()\n",
        "\n",
        "# Step 2: Create a popularity label\n",
        "merged_data['Cuisine_Popularity'] = merged_data['Cuisines'].apply(\n",
        "    lambda x: 'Popular' if cuisine_counts.get(x, 0) >= median_count else 'Less_Popular'\n",
        ")\n",
        "\n",
        "# Step 3: Get ratings for both groups\n",
        "popular_ratings = merged_data[merged_data['Cuisine_Popularity'] == 'Popular']['Rating']\n",
        "less_popular_ratings = merged_data[merged_data['Cuisine_Popularity'] == 'Less_Popular']['Rating']\n",
        "\n",
        "# Step 4: Mann-Whitney U Test (non-parametric)\n",
        "stat, p_val = mannwhitneyu(popular_ratings, less_popular_ratings, alternative='two-sided')\n",
        "\n",
        "print(f\"Mann-Whitney U Statistic: {stat}\")\n",
        "print(f\"P-Value: {p_val:.4f}\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Used: Mann-Whitney U Test"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're comparing average ratings between two groups: Popular Cuisines vs Less Popular Cuisines"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis -\n",
        "* H₀: Average rating is the same across all cost buckets.\n",
        "\n",
        "Alternate Hypothesis -\n",
        "* H₁: Average rating differs significantly among different cost buckets.\n"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Step 1: Create Cost Buckets\n",
        "def cost_bucket(cost):\n",
        "    if cost < 500:\n",
        "        return '<500'\n",
        "    elif cost < 1000:\n",
        "        return '500-1k'\n",
        "    elif cost < 1500:\n",
        "        return '1k-1.5k'\n",
        "    elif cost < 2000:\n",
        "        return '1.5k-2k'\n",
        "    else:\n",
        "        return '2k+'\n",
        "\n",
        "merged_data['Cost_Bucket'] = merged_data['Cost'].apply(cost_bucket)\n",
        "\n",
        "# Step 2: Group ratings by cost bucket\n",
        "grouped_ratings = merged_data.groupby('Cost_Bucket')['Rating'].apply(list)\n",
        "\n",
        "# Step 3: Perform One-Way ANOVA\n",
        "f_stat, p_val = f_oneway(*grouped_ratings)\n",
        "\n",
        "print(f\"F-Statistic: {f_stat:.3f}\")\n",
        "print(f\"P-Value: {p_val:.4f}\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Used: One-Way ANOVA"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* You’re comparing average ratings across multiple cost buckets (e.g., <₹500, ₹500–₹1k, ₹1k–1.5k, etc.).\n",
        "\n",
        "* ANOVA is appropriate for comparing >2 groups.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "merged_data.isnull().sum() # so there is no missing values"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare the dataset for analysis, I applied a combination of missing value handling techniques. For critical columns such as Reviewer, Review, Rating, and Time, rows with missing values were dropped to ensure data quality and avoid unreliable imputations. For categorical fields like Collections and Timings, missing values were filled with explicit placeholder labels (‘Not Specified’ and ‘Not Available’ respectively) to maintain dataset completeness while clearly marking missing information. This balanced approach preserves the integrity of key data points while minimizing data loss."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "def detect_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
        "    print(f\"Outliers detected in '{column}': {len(outliers)}\")\n",
        "    return outliers\n",
        "\n",
        "cost_outliers = detect_outliers_iqr(merged_data, 'Cost')\n",
        "\n",
        "rating_outliers = detect_outliers_iqr(merged_data, 'Rating')\n",
        "\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Interquartile Range (IQR) method to identify outliers in numerical columns such as Cost and Rating. This technique highlights values that fall significantly outside the typical range without removing or modifying them. I chose the IQR method because it is robust to skewed distributions and provides a reliable way to detect extreme values for further analysis."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import re\n",
        "\n",
        "# Step 1: Split cuisines into lists\n",
        "data1['Cuisine_List'] = data1['Cuisines'].apply(lambda x: [i.strip() for i in x.split(',')])\n",
        "\n",
        "# Step 2: One-hot encode cuisines\n",
        "mlb = MultiLabelBinarizer()\n",
        "cuisine_encoded = pd.DataFrame(mlb.fit_transform(data1['Cuisine_List']), columns=mlb.classes_)\n",
        "\n",
        "# Step 3: Define timing encoding function\n",
        "def encode_timing(t):\n",
        "    if pd.isnull(t):\n",
        "        return 'Unknown'\n",
        "    t = t.lower()\n",
        "    if 'am' in t and 'pm' in t:\n",
        "        return 'All day'\n",
        "    elif re.search(r'(\\d{1,2})(:?\\d{0,2})?\\s*am', t):\n",
        "        return 'Morning'\n",
        "    elif re.search(r'(\\d{1,2})(:?\\d{0,2})?\\s*pm', t):\n",
        "        return 'Evening'\n",
        "    return 'Unknown'\n",
        "\n",
        "# Step 4: Apply timing encoding\n",
        "data1['Timing_Category'] = data1['Timings'].apply(encode_timing)\n",
        "\n",
        "# Step 5: Combine encoded cuisines and timing dummies with original data\n",
        "data1_encoded = pd.concat([data1.drop(columns=['Cuisine_List']), cuisine_encoded], axis=1)\n",
        "data1_encoded = pd.concat([data1_encoded, pd.get_dummies(data1_encoded['Timing_Category'], prefix='Timing').astype(int)], axis=1)\n",
        "\n",
        "# Step 6: Drop the 'Timing_Category' column as it is now encoded\n",
        "data1_encoded.drop(columns=['Timing_Category'], inplace=True)\n",
        "\n",
        "# View the first few rows\n",
        "print(data1_encoded.head())\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used One-Hot Encoding to transform categorical variables like cuisines and timing categories into binary indicator columns. This technique was chosen because it effectively converts nominal categorical data without implying any ordinal relationship, allowing machine learning models to interpret each category independently and avoid introducing unintended hierarchy."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "!pip install contractions\n",
        "\n",
        "import contractions\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "merged_data['Review_Expanded'] = merged_data['Review'].apply(expand_contractions)\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "merged_data['Review_Lower'] = merged_data['Review_Expanded'].str.lower()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "merged_data['Review_NoPunct'] = merged_data['Review_Lower'].apply(remove_punctuation)\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_urls(text):\n",
        "    return re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "\n",
        "def remove_words_with_digits(text):\n",
        "    return ' '.join([word for word in text.split() if not any(char.isdigit() for char in word)])\n"
      ],
      "metadata": {
        "id": "ZGve3VeSN_UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "merged_data['Review_Cleaned'] = merged_data['Review_NoPunct'].apply(remove_urls)\n",
        "\n",
        "merged_data['Review_Cleaned'] = merged_data['Review_Cleaned'].apply(remove_words_with_digits)\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "merged_data['Review_Cleaned'] = merged_data['Review_Cleaned'].apply(remove_stopwords)\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "def remove_extra_whitespace(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "merged_data['Review_Cleaned'] = merged_data['Review_Cleaned'].apply(remove_extra_whitespace)\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def synonym_replace(text):\n",
        "    new_words = []\n",
        "    for word in text.split():\n",
        "        syns = wordnet.synsets(word)\n",
        "        if syns:\n",
        "            synonym = syns[0].lemmas()[0].name()\n",
        "            new_words.append(synonym.replace('_', ' '))\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "merged_data['Review_Rephrased'] = merged_data['Review_Cleaned'].apply(synonym_replace)\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "merged_data['Review_Tokens'] = merged_data['Review_Cleaned'].apply(word_tokenize)\n"
      ],
      "metadata": {
        "id": "jxGoY3ExOuCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def stem_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmed = [ps.stem(word) for word in tokens]\n",
        "    return ' '.join(stemmed)\n",
        "\n",
        "merged_data['Review_Stemmed'] = merged_data['Review_Cleaned'].apply(stem_text)\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used stemming as the text normalization technique to reduce words to their root forms. Stemming simplifies the vocabulary by cutting words to their base stems, which helps in reducing dimensionality and improving the efficiency of text analysis. Although stemming can produce non-dictionary words, it is computationally faster and suitable for tasks where exact word meaning is less critical."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS tagging\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "\n",
        "# Define the sentiment function\n",
        "def get_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    sentiment = (\n",
        "        'Positive' if polarity > 0 else\n",
        "        'Negative' if polarity < 0 else\n",
        "        'Neutral'\n",
        "    )\n",
        "    return pd.Series([sentiment, polarity])\n",
        "\n",
        "# Applied it to our cleaned review column\n",
        "merged_data[['Sentiment', 'Sentiment_Score']] = merged_data['Review_Cleaned'].apply(get_sentiment)\n",
        "\n",
        "# Display a sample of results\n",
        "print(merged_data[['Review_Cleaned', 'Sentiment', 'Sentiment_Score']].head())\n"
      ],
      "metadata": {
        "id": "DFBmMIW31_Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "\n",
        "X_tfidf = tfidf.fit_transform(merged_data['Review_Cleaned'])\n",
        "\n",
        "print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used TF-IDF vectorization to convert text into numerical features. TF-IDF effectively captures the importance of words by balancing their frequency within a document against how common they are across the entire dataset. This helps highlight distinctive words while reducing the impact of common, less informative terms, making it well-suited for tasks like classification and clustering."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def get_sentiment_score(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "data2_encoded['Sentiment_Score'] = data2_encoded['Review'].apply(get_sentiment_score)\n"
      ],
      "metadata": {
        "id": "7xi6mWsV7dUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data transformation\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from textblob import TextBlob\n",
        "\n",
        "# data processing\n",
        "\n",
        "# 1. Split cuisines into lists\n",
        "data1['Cuisine_List'] = data1['Cuisines'].apply(lambda x: [c.strip() for c in x.split(',')])\n",
        "\n",
        "# 2. One-hot encode cuisines\n",
        "mlb = MultiLabelBinarizer()\n",
        "cuisine_encoded = pd.DataFrame(mlb.fit_transform(data1['Cuisine_List']), columns=mlb.classes_)\n",
        "\n",
        "# 3. Function to encode timings into categories\n",
        "def encode_timing(t):\n",
        "    if pd.isnull(t):\n",
        "        return 'Unknown'\n",
        "    t = t.lower()\n",
        "    if 'am' in t and 'pm' in t:\n",
        "        return 'All day'\n",
        "    elif re.search(r'(\\d{1,2})(:?\\d{0,2})?\\s*am', t):\n",
        "        return 'Morning'\n",
        "    elif re.search(r'(\\d{1,2})(:?\\d{0,2})?\\s*pm', t):\n",
        "        return 'Evening'\n",
        "    return 'Unknown'\n",
        "\n",
        "# 4. Apply timing encoding\n",
        "data1['Timing_Category'] = data1['Timings'].apply(encode_timing)\n",
        "\n",
        "# 5. Create timing dummies\n",
        "timing_dummies = pd.get_dummies(data1['Timing_Category'], prefix='Timing')\n",
        "\n",
        "# 6. Combine all features into data1_encoded\n",
        "data1_encoded = pd.concat([\n",
        "    data1.drop(columns=['Cuisine_List', 'Timing_Category', 'Cuisines', 'Timings']),\n",
        "    cuisine_encoded,\n",
        "    timing_dummies\n",
        "], axis=1)\n",
        "\n",
        "# data processing\n",
        "\n",
        "data2_encoded = data2.copy()\n",
        "\n",
        "# 7. Extract Review_Count and Follower_Count from Metadata text\n",
        "def extract_metadata(meta):\n",
        "    review_count = 0\n",
        "    follower_count = 0\n",
        "    if isinstance(meta, str):\n",
        "        parts = meta.split(',')\n",
        "        for p in parts:\n",
        "            p_lower = p.lower()\n",
        "            if 'review' in p_lower:\n",
        "                digits = ''.join(filter(str.isdigit, p))\n",
        "                review_count = int(digits) if digits else 0\n",
        "            elif 'follower' in p_lower:\n",
        "                digits = ''.join(filter(str.isdigit, p))\n",
        "                follower_count = int(digits) if digits else 0\n",
        "    return pd.Series([review_count, follower_count])\n",
        "\n",
        "data2_encoded[['Review_Count', 'Follower_Count']] = data2_encoded['Metadata'].apply(extract_metadata)\n",
        "\n",
        "# 8. Add Sentiment Score to data2_encoded\n",
        "def get_sentiment_score(text):\n",
        "    if pd.isnull(text):\n",
        "        return 0.0\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "data2_encoded['Sentiment_Score'] = data2_encoded['Review'].apply(get_sentiment_score)\n",
        "\n",
        "agg_reviews = data2_encoded.groupby('Restaurant').agg({\n",
        "    'Rating': 'mean',\n",
        "    'Pictures': 'sum',\n",
        "    'Review_Count': 'mean',\n",
        "    'Follower_Count': 'mean',\n",
        "    'Sentiment_Score': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# 9. Rename 'Restaurant' to 'Name' for merging\n",
        "agg_reviews.rename(columns={'Restaurant': 'Name'}, inplace=True)\n",
        "\n",
        "cluster_ready_df = pd.merge(data1_encoded, agg_reviews, on='Name', how='inner')\n",
        "\n",
        "# Check final dataframe\n",
        "print(cluster_ready_df.head())\n",
        "print(cluster_ready_df.columns)\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Assuming 'cluster_ready_df' is your prepared dataframe (all numeric features)\n",
        "\n",
        "# Step 1: Select features (exclude non-numeric columns if any)\n",
        "X = cluster_ready_df.select_dtypes(include=['number'])\n",
        "\n",
        "# Step 2: Initialize KMeans (choose number of clusters)\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "\n",
        "# Step 3: Fit the model and predict cluster labels\n",
        "cluster_labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Step 4: Add cluster labels to dataframe\n",
        "cluster_ready_df['KMeans_Cluster'] = cluster_labels\n",
        "\n",
        "# Step 5: See cluster counts\n",
        "print(cluster_ready_df['KMeans_Cluster'].value_counts())\n",
        "\n",
        "# Optional: Inspect cluster centers\n",
        "print(kmeans.cluster_centers_)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Assuming your data is in `X` (numpy array or DataFrame, scaled)\n",
        "range_n_clusters = range(2, 11)\n",
        "silhouette_scores = []\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(X)\n",
        "    score = silhouette_score(X, cluster_labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range_n_clusters, silhouette_scores, marker='o')\n",
        "plt.title('Silhouette Score for different numbers of clusters')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.xticks(range_n_clusters)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(cluster_ready_df.shape)  # Should be (100, ?)\n",
        "print(X_scaled.shape)           # Should be (100, number_of_features)\n"
      ],
      "metadata": {
        "id": "sQZWgSeZAARB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Fit the Algorithm\n",
        "# Predict on the model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Select numeric features from cluster_ready_df\n",
        "X = cluster_ready_df.select_dtypes(include=['number'])\n",
        "\n",
        "# Step 2: Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "cluster_ready_df['KMeans_Cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Step 4: Reduce to 2D using PCA for visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Step 5: Plot the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_ready_df['KMeans_Cluster'], cmap='rainbow', s=40, alpha=0.7)\n",
        "plt.title(\"KMeans Clustering Results (PCA Projection)\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rnimbe4W-RJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " used a manual hyperparameter tuning approach with KMeans clustering, where I selected the number of clusters (n_clusters=2) based on prior domain knowledge or exploratory analysis. Instead of automated search methods like GridSearchCV or RandomizedSearchCV, I chose to scale features using StandardScaler to normalize the data, improving cluster performance.\n",
        "\n",
        "This approach is appropriate because KMeans clustering is unsupervised, and common hyperparameter tuning techniques like GridSearchCV are not directly applicable. Instead, domain expertise combined with visualizations helps in selecting an optimal number of clusters."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling features and using KMeans improved cluster quality, reflected in a higher silhouette score. The updated silhouette score and PCA plot show clearer, well-separated clusters, enabling better customer segmentation and business insights."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Initialize Agglomerative Clustering (use 'metric' instead of 'affinity')\n",
        "agg_clust = AgglomerativeClustering(n_clusters=2, metric='euclidean', linkage='ward')\n",
        "\n",
        "# Fit and predict clusters\n",
        "cluster_ready_df['Agglomerative_Cluster'] = agg_clust.fit_predict(X_scaled)\n",
        "\n",
        "# Display cluster counts\n",
        "print(cluster_ready_df['Agglomerative_Cluster'].value_counts())\n",
        "\n",
        "# Plot dendrogram for visualization\n",
        "linked = linkage(X_scaled, method='ward')\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=False)\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('Samples')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qjBmZP9YAwZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "silhouette_scores = []\n",
        "cluster_range = range(2, 10)  # Test cluster counts from 2 to 9\n",
        "\n",
        "for n_clusters in cluster_range:\n",
        "    model = AgglomerativeClustering(n_clusters=n_clusters, metric='euclidean', linkage='ward')\n",
        "    cluster_labels = model.fit_predict(X_scaled)\n",
        "    score = silhouette_score(X_scaled, cluster_labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(cluster_range, silhouette_scores, marker='o')\n",
        "plt.title('Silhouette Score for Different Numbers of Clusters')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try different K values and evaluate silhouette score\n",
        "for k in range(2, 11):\n",
        "    model = AgglomerativeClustering(n_clusters=k)\n",
        "    labels = model.fit_predict(X_scaled)\n",
        "    score = silhouette_score(X_scaled, labels)\n",
        "    print(f\"K={k}, Silhouette Score={score:.3f}\")"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used manual tuning with Silhouette Score evaluation, by trying different values of k (number of clusters) in the range of 2 to 10.\n",
        "\n",
        "Why this technique?\n",
        "\n",
        "* For unsupervised learning, traditional GridSearchCV or RandomSearchCV are not directly applicable as there's no ground truth.\n",
        "* Instead, we use an internal evaluation metric—the Silhouette Score to assess the quality of clustering.\n",
        "* This method allows us to empirically choose the best k (number of clusters) that gives the highest silhouette score, indicating well-separated and dense clusters.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, improvement was observed.\n",
        "\n",
        "* The optimal number of clusters is K = 3, with the highest Silhouette Score of 0.51.\n",
        "\n",
        "* This indicates a meaningful improvement in clustering performance compared to default or arbitrary choices of k.\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Silhouette Score: Measures how well a restaurant fits within its cluster vs. other clusters.\n",
        "\n",
        "* High Score Meaning: Indicates strong, well separated clusters with similar restaurant traits.\n",
        "\n",
        "* Customer Segmentation: Helps identify distinct groups based on restaurant characteristics and customer reviews.\n",
        "\n",
        "* Menu Optimization: Enables targeting clusters with popular food preferences.\n",
        "\n",
        "* Operational Planning: Clusters based on timing support shift and resource optimization.\n",
        "\n",
        "* Marketing Strategy: Facilitates location-based and preference-based promotions tailored to each cluster."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# we try to fit the DBSCAN model\n",
        "dbscan = DBSCAN(eps=1.5, min_samples=5)\n",
        "cluster_ready_df['DBSCAN_Cluster'] = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# predict the model\n",
        "print(cluster_ready_df['DBSCAN_Cluster'].value_counts())\n",
        "\n",
        "# evaluate the clustering\n",
        "n_clusters = len(set(dbscan.labels_)) - (1 if -1 in dbscan.labels_ else 0)\n",
        "if n_clusters > 1:\n",
        "    score = silhouette_score(X_scaled, dbscan.labels_)\n",
        "    print(f\"Silhouette Score: {score:.4f}\")\n",
        "else:\n",
        "    print(\"Silhouette Score not available (only one cluster found).\")\n",
        "\n",
        "# visualize using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_ready_df['DBSCAN_Cluster'], cmap='rainbow', s=40, alpha=0.7)\n",
        "plt.title(\"DBSCAN Clustering Results (PCA Projection)\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "eps_values = np.arange(0.5, 3.0, 0.2)\n",
        "silhouette_scores = []\n",
        "\n",
        "for eps in eps_values:\n",
        "    db = DBSCAN(eps=eps, min_samples=5)\n",
        "    labels = db.fit_predict(X_scaled)\n",
        "\n",
        "    if len(set(labels)) > 1 and len(set(labels)) != 1 + (1 if -1 in labels else 0):\n",
        "        score = silhouette_score(X_scaled, labels)\n",
        "        silhouette_scores.append(score)\n",
        "    else:\n",
        "        silhouette_scores.append(-1)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(eps_values, silhouette_scores, marker='o', linestyle='--', color='teal')\n",
        "plt.title(\"Silhouette Score vs Epsilon for DBSCAN\")\n",
        "plt.xlabel(\"Epsilon (eps)\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Try different eps values and check silhouette score\n",
        "for eps in [1.0, 1.5, 2.0, 2.5, 3.0]:\n",
        "    model = DBSCAN(eps=eps, min_samples=5)\n",
        "    labels = model.fit_predict(X_scaled)\n",
        "\n",
        "    # Count number of clusters\n",
        "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "\n",
        "    # Only evaluate if at least 2 clusters exist\n",
        "    if n_clusters > 1:\n",
        "        score = silhouette_score(X_scaled, labels)\n",
        "        print(f\"eps={eps}, Silhouette Score={score:.3f}, Clusters={n_clusters}\")\n",
        "    else:\n",
        "        print(f\"eps={eps}, Not enough clusters to evaluate (Clusters={n_clusters})\")\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used manual grid search to tune the eps parameter in the DBSCAN model, as DBSCAN doesn’t require a predefined number of clusters and isn’t compatible with supervised optimization methods like GridSearchCV. We tested a range of eps values (from 1.0 to 3.0) while keeping min_samples constant at 5, and evaluated each using the silhouette score to determine clustering quality. This approach is ideal for DBSCAN due to its unsupervised nature and the limited number of hyperparameters."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite tuning the eps hyperparameter manually, DBSCAN failed to generate meaningful clusters in this case. Either all data points were treated as noise or combined into a single cluster. Therefore, no improvement was observed, and silhouette score couldn’t be evaluated. This indicates DBSCAN may not be suitable for this dataset's structure, possibly due to its high dimensionality or overlapping density regions."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key evaluation metric used across all three models was the Silhouette Score because it shows how well defined and distinct the clusters are. A higher score means clearer groupings, which helps the business target customer segments effectively. This metric ensures the clustering results are meaningful and actionable for better marketing and operations."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose KMeans as the final clustering model because it provided the most balanced and interpretable clusters with a good Silhouette Score. Unlike DBSCAN, which struggled to find meaningful clusters on this dataset, and Agglomerative Clustering, which had similar but slightly less clear separation, KMeans offered consistent and stable clusters that are easier to use for business segmentation and actionable insights."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used KMeans clustering, which groups restaurants based on similar features by minimizing distances to cluster centers. Since it’s unsupervised, I approximated feature importance using tools like SHAP on a supervised model trained to predict clusters. Key features influencing clusters included cuisine types, timing, ratings, review counts, and sentiment scores. This helps identify what drives differences between restaurant groups for better business decisions."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we leveraged unsupervised machine learning models to perform customer and restaurant segmentation using a rich dataset containing diverse features such as cuisine types, operating timings, customer reviews, ratings, and sentiment scores. Through careful data preprocessing, including categorical encoding of cuisines and timings, and feature engineering from metadata, we prepared a comprehensive dataset suitable for clustering analysis.\n",
        "\n",
        "We implemented multiple clustering algorithms primarily KMeans, Agglomerative Clustering, and DBSCAN to identify natural groupings within the data without prior labeling. Among these, KMeans provided the most interpretable and actionable clusters, as validated by silhouette scores and visualized through PCA plots. Agglomerative Clustering gave complementary insights, while DBSCAN’s density based approach was limited by the data distribution and parameter sensitivity.\n",
        "\n",
        "The clusters derived from KMeans revealed meaningful patterns in restaurant characteristics, including cuisine preferences, pricing, and customer sentiment. This segmentation enables targeted business strategies such as personalized marketing campaigns, menu optimization tailored to cluster preferences, and improved operational efficiency through better staffing and scheduling aligned with customer behavior.\n",
        "\n",
        "From a business perspective, these clusters allow for refined customer engagement by understanding which groups prefer specific cuisines or dining times, and how sentiment varies across clusters. The insights can drive revenue growth, enhance customer satisfaction, and optimize resource allocation.\n",
        "\n",
        "Overall, the project demonstrated the value of unsupervised learning for market segmentation in the food service industry. Future work could incorporate additional data sources, refine hyperparameter tuning, and apply advanced explainability techniques to further improve model interpretability and business impact."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}